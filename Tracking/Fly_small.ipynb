{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tracktor as tr\n",
    "import cv2\n",
    "import sys\n",
    "import scipy.signal\n",
    "import os\n",
    "import random\n",
    "\n",
    "Ffmpeg = \"/usr/local/Cellar/ffmpeg/5.0-with-options_1/bin/ffmpeg\" # Specifies where to look for shell ffmpeg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Path definition and video trimming\n",
    "The cells below define the target video path and trim it according to visual assessment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# name of source video and paths\n",
    "path = '/Volumes/Universal_HD/VideoAnalysis/GatedArenas_Agar/220715/Trained_Agar/091847_s0a0_p0-0/'\n",
    "source = 'Trained_Agar_Starved_noWater_p0-0_80fps'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "startpoint = \"00:00:01\" # Start point is the timepoint where the arena is fully opened and static\n",
    "finishpoint = \"00:10:00\" # Finish point is the last timepoint of the video\n",
    "\n",
    "os.system(#Ffmpeg +\n",
    "     \"ffmpeg -i \" + path + source + \".mp4 -ss \" + startpoint + \" -to \" + finishpoint + \" -c copy \" + path + source + \"_Trimmed.mp4\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "video = source + \"_Trimmed\"\n",
    "\n",
    "input_vidpath = path + video + '.mp4'\n",
    "output_vidpath = path + video + '_tracked.mp4'\n",
    "output_filepath = path + video + '_tracked.csv'\n",
    "codec = 'mp4v' # try other codecs if the default doesn't work ('DIVX', 'avc1', 'XVID') note: this list is non-exhaustive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# colours is a vector of BGR values which are used to identify individuals in the video\n",
    "# since we only have one individual, the program will only use the first element from this array i.e. (0,0,255) - red\n",
    "# number of elements in colours should be greater than n_inds (THIS IS NECESSARY FOR VISUALISATION ONLY)\n",
    "n_inds = 1\n",
    "colours = [(0,0,255),(0,255,255),(255,0,255),(255,255,255),(255,255,0),(255,0,0),(0,255,0),(0,0,0)]\n",
    "\n",
    "# this is the block_size and offset used for adaptive thresholding (block_size should always be odd)\n",
    "# these values are critical for tracking performance\n",
    "#block_size = 35 # These should be only set if using adaptive thresholding\n",
    "#offset = 10\n",
    "\n",
    "# the scaling parameter can be used to speed up tracking if video resolution is too high (use value 0-1)\n",
    "scaling = 1.0\n",
    "\n",
    "# minimum area and maximum area occupied by the animal in number of pixels\n",
    "# this parameter is used to get rid of other objects in view that might be hard to threshold out but are differently sized\n",
    "min_area = 470\n",
    "max_area = 8000\n",
    "\n",
    "# mot determines whether the tracker is being used in noisy conditions to track a single object or for multi-object\n",
    "# using this will enable k-means clustering to force n_inds number of animals\n",
    "mot = False\n",
    "\n",
    "# kernel for erosion and dilation\n",
    "# useful since thin spider limbs are sometimes detected as separate objects\n",
    "kernel = np.ones((5,5),np.uint8)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Background definition\n",
    "The cell below produces a background frame that can be used to 1) locate the food objects in the arena 2) background subtraction for subsequent tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fgbg5 = cv2.bgsegm.createBackgroundSubtractorGSOC(\n",
    "                                                  #nSamples=2,\n",
    "                                                  #replaceRate=0.900,\n",
    "                                                  #propagationRate=0.003 ,\n",
    "                                                  #noiseRemovalThresholdFacFG = 0.45,\n",
    "                                                  #noiseRemovalThresholdFacBG= 0.45,\n",
    "                                                  #hitsThreshold=5\n",
    "                                                  )#default : 0.003\n",
    "#fgbg4 = cv2.bgsegm.createBackgroundSubtractorCNT(\n",
    "                                                  #nSamples=100,\n",
    "                                                  #replaceRate=0.900,\n",
    "                                                  #propagationRate=0.003 ,\n",
    "                                                  #noiseRemovalThresholdFacFG = 0.45,\n",
    "                                                  #noiseRemovalThresholdFacBG= 0.45,\n",
    "                                                  #hitsThreshold=5\n",
    "                                                  #)#default : 0.003\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(input_vidpath)\n",
    "\n",
    "# Set framesize as the same one as the images read from input video\n",
    "BG_framesize = (int(cap.read()[1].shape[1]*scaling),int(cap.read()[1].shape[0]*scaling))\n",
    "\n",
    "fourcc = cv2.VideoWriter_fourcc(*codec)\n",
    "# Create a Video writer with the desired parameters\n",
    "Background_Generator = cv2.VideoWriter(filename = path+'Background_Generator.mp4',\n",
    "                                       fourcc = fourcc,\n",
    "                                       fps = 80,\n",
    "                                       frameSize = BG_framesize,\n",
    "                                       #isColor=True,\n",
    "                                       )\n",
    "\n",
    "# Write a video with random frames taken in the input video\n",
    "f = 0\n",
    "while f <= 300 :\n",
    "    # get total number of frames\n",
    "    totalFrames = cap.get(cv2.CAP_PROP_FRAME_COUNT)\n",
    "    randomFrameNumber=random.randint(0, totalFrames)\n",
    "    # set frame position\n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES,randomFrameNumber)\n",
    "    success, image = cap.read()\n",
    "\n",
    "    if success:\n",
    "\n",
    "        Background_Generator.write(image)\n",
    "\n",
    "    f += 1\n",
    "\n",
    "cap.release()\n",
    "Background_Generator.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Adjust using live rendering to get a clean background image. Default : 500\n",
    "\n",
    "cap = cv2.VideoCapture(path+'Background_Generator.mp4')\n",
    "\n",
    "target = 0\n",
    "cap.set(1, target) # Set the starting point, try to find a section where the fly moves a lot.\n",
    "\n",
    "Frame = target + 300\n",
    "\n",
    "while(1):\n",
    "\t# read frames\n",
    "    ret, img = cap.read()\n",
    "    this = cap.get(1)\n",
    "\n",
    "    # apply mask for background subtraction\n",
    "\n",
    "    fgmask5 = fgbg5.apply(img)\n",
    "\n",
    "    bg = fgbg5.getBackgroundImage()\n",
    "\n",
    "    cv2.imshow('Original', img);\n",
    "    cv2.imshow('GSOC', fgmask5)\n",
    "    cv2.imshow('background', bg)\n",
    "    subtracted = cv2.absdiff(img, bg)\n",
    "\n",
    "    if this == Frame:\n",
    "\n",
    "        cv2.imwrite(path+'Background.jpg', bg)\n",
    "        break\n",
    "\n",
    "    k = cv2.waitKey(30) & 0xff\n",
    "    if k == 27:\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)\n",
    "\n",
    "Background = cv2.imread(path+'Background.jpg')\n",
    "os.remove(path+'Background_Generator.mp4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    " # Adjust using live rendering to get a clean background image. Default : 500\n",
    "\n",
    "cap = cv2.VideoCapture(input_vidpath)\n",
    "\n",
    "target = 4500\n",
    "cap.set(1, target) # Set the starting point, try to find a section where the fly moves a lot.\n",
    "\n",
    "Frame = target + 300\n",
    "\n",
    "while(1):\n",
    "\t# read frames\n",
    "    ret, img = cap.read()\n",
    "    this = cap.get(1)\n",
    "\n",
    "    # apply mask for background subtraction\n",
    "\n",
    "    #fgmask4 = fgbg4.apply(img)\n",
    "    fgmask5 = fgbg5.apply(img)\n",
    "\n",
    "    #fgmask6 = fgbg6.apply(img)\n",
    "\n",
    "\n",
    "    bg = fgbg5.getBackgroundImage()\n",
    "    #bg = cv2.erode(bg, kernel, iterations = 1) # Use this only if necessary (e.g. Fly is too static, can't get cropped out easily)\n",
    "    #bg = cv2.dilate(bg, kernel, iterations = 2)\n",
    "    #bg2 = fgbg4.getBackgroundImage()\n",
    "\n",
    "    #\n",
    "    #bg2 = fgbg4.getBackgroundImage()\n",
    "    #font = cv2.FONT_HERSHEY_SCRIPT_SIMPLEX\n",
    "    #cv2.putText(img, str(int(this)), (5,30), font, 1, (255,255,255), 2)\n",
    "\n",
    "    cv2.imshow('Original', img);\n",
    "    #cv2.imshow('MOG', fgmask1);\n",
    "    #cv2.imshow('MOG2', fgmask2);\n",
    "    #cv2.imshow('GMG', fgmask3);\n",
    "    #cv2.imshow('CNT', fgmask4)\n",
    "    cv2.imshow('GSOC', fgmask5)\n",
    "    #cv2.imshow('LSBP', fgmask6)\n",
    "    cv2.imshow('background', bg)\n",
    "    #cv2.imshow('backgroundCNT', bg2)\n",
    "    subtracted = cv2.absdiff(img, bg)\n",
    "\n",
    "    if this == Frame:\n",
    "\n",
    "        cv2.imwrite(path+'Background.jpg', bg)\n",
    "        #cv2.imwrite(path+'Full.jpg', img)\n",
    "        #cv2.imwrite(path+'Subtracted.jpg', subtracted)\n",
    "        break\n",
    "\n",
    "    k = cv2.waitKey(30) & 0xff\n",
    "    if k == 27:\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "Background = cv2.imread(path+'Background.jpg')\n",
    "#Full = cv2.imread(path+'Full.jpg')\n",
    "#Subtracted = cv2.imread(path+'Subtracted.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "Background = cv2.imread(path+'Background.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Tracking\n",
    "The cell below runs the tracking code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "## Open video\n",
    "cap = cv2.VideoCapture(input_vidpath)\n",
    "if cap.isOpened() == False:\n",
    "    sys.exit('Video file cannot be read! Please check input_vidpath to ensure it is correctly pointing to the video file')\n",
    "\n",
    "## Video writer class to output video with contour and centroid of tracked object(s)\n",
    "# make sure the frame size matches size of array 'final'\n",
    "fourcc = cv2.VideoWriter_fourcc(*codec)\n",
    "output_framesize = (int(cap.read()[1].shape[1]*scaling),int(cap.read()[1].shape[0]*scaling))\n",
    "out = cv2.VideoWriter(filename = output_vidpath, fourcc = fourcc, fps = 80.0, frameSize = output_framesize, isColor = True)\n",
    "\n",
    "## Individual location(s) measured in the last and current step\n",
    "meas_last = list(np.zeros((n_inds,2)))\n",
    "meas_now = list(np.zeros((n_inds,2)))\n",
    "\n",
    "last = 0\n",
    "df = []\n",
    "\n",
    "Background = cv2.imread(path+'Background.jpg')\n",
    "\n",
    "while(True):\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    this = cap.get(1)\n",
    "    if ret == True:\n",
    "        frame = cv2.resize(frame, None, fx = scaling, fy = scaling, interpolation = cv2.INTER_LINEAR)\n",
    "        subtracted = cv2.absdiff(frame, Background)\n",
    "        thresh = tr.colour_to_thresh_binary(subtracted, 22)\n",
    "\n",
    "        #thresh = cv2.erode(thresh, kernel, iterations = 1)\n",
    "        #thresh = cv2.dilate(thresh, kernel, iterations = 1)\n",
    "        final, contours, meas_last, meas_now = tr.detect_and_draw_contours(frame, thresh, meas_last, meas_now, min_area, max_area)\n",
    "        row_ind, col_ind = tr.hungarian_algorithm(meas_last, meas_now)\n",
    "        final, meas_now, df = tr.reorder_and_draw(final, colours, n_inds, col_ind, meas_now, df, mot, this)\n",
    "        \n",
    "        # Create output dataframe\n",
    "        for i in range(n_inds):\n",
    "            df.append([this, meas_now[i][0], meas_now[i][1]])\n",
    "        \n",
    "        # Display the resulting frame\n",
    "        out.write(final)\n",
    "        cv2.imshow('frame', final)\n",
    "        cv2.imshow('subtracted', subtracted)\n",
    "        if cv2.waitKey(1) == 27 or meas_now[0][0] < 20 or meas_now[0][0] > cap.get(3) - 20 or meas_now[0][1] < 20 or meas_now[0][1] > cap.get(4) - 20:\n",
    "            break\n",
    "            \n",
    "    if last >= this:\n",
    "        break\n",
    "    \n",
    "    last = this\n",
    "\n",
    "## Write positions to file\n",
    "df = pd.DataFrame(np.matrix(df), columns = ['frame','pos_x','pos_y'])\n",
    "df.to_csv(output_filepath, sep=',')\n",
    "\n",
    "## When everything done, release the capture\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Summary statistics\n",
    "The cells below provide functions to perform basic summary statistics - in this case, trajectory, distance moved between successive frames, cumulative distance within a time-window, velocity and acceleration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(output_filepath)\n",
    "df.head()\n",
    "os.mkdir(path = path+'figs/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.scatter(df['pos_x'], df['pos_y'], c=df['frame'], alpha=0.5)\n",
    "plt.xlabel('X', fontsize=16)\n",
    "plt.ylabel('Y', fontsize=16)\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.savefig(path+'figs/trajectory.eps', format='eps', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.hist2d(df['pos_x'], df['pos_y'], bins=30)\n",
    "plt.xlabel('X', fontsize=16)\n",
    "plt.ylabel('Y', fontsize=16)\n",
    "plt.gca().invert_yaxis()\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(path+'figs/heatmap.eps', format='eps', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "## Parameters like speed and acceleration can be very noisy. Small noise in positional data is amplified as we take the\n",
    "## derivative to get speed and acceleration. We therefore smooth this data to obtain reliable values and eliminate noise.\n",
    "\n",
    "# the smoothing window parameter determines the extent of smoothing (this parameter must be odd)\n",
    "smoothing_window = 11\n",
    "\n",
    "## Fill in the parameters below if you'd like movement measures to be converted from pixels and frames to \n",
    "## real-world measures (cms and secs)\n",
    "\n",
    "# Frame-rate (fps or frames per second) of recorded video to calculate time\n",
    "fps = 80\n",
    "\n",
    "# Pixels per cm to in the recorded video to calculate distances\n",
    "pxpercm = 254 * scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dx = df['pos_x'] - df['pos_x'].shift(n_inds)\n",
    "dy = df['pos_y'] - df['pos_y'].shift(n_inds)\n",
    "d2x = dx - dx.shift(1)\n",
    "d2y = dy - dy.shift(1)\n",
    "df['speed'] = np.sqrt(dx**2 + dy**2)\n",
    "df['smoothed_speed'] = scipy.signal.savgol_filter(df['speed'], smoothing_window, 1)\n",
    "df['accn'] = np.sqrt(d2x**2 + d2y**2)\n",
    "df['smoothed_accn'] = scipy.signal.savgol_filter(df['accn'], smoothing_window, 1)\n",
    "df['cum_dist'] = df['smoothed_speed'].cumsum()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def cumul_dist(start_fr, end_fr):\n",
    "    if start_fr != 1:\n",
    "        cumul_dist = df['cum_dist'][df['frame'] == end_fr].values[0] - df['cum_dist'][df['frame'] == start_fr].values[0]\n",
    "    else:\n",
    "        cumul_dist = df['cum_dist'][df['frame'] == end_fr].values[0]\n",
    "    return cumul_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "cumul_dist(150,200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df['time'] = df['frame'] / fps\n",
    "df['speed'] = df['speed'] * fps / pxpercm\n",
    "df['smoothed_speed'] = df['smoothed_speed'] * fps / pxpercm\n",
    "df['accn'] = df['accn'] * fps * fps / pxpercm\n",
    "df['smoothed_accn'] = df['smoothed_accn'] * fps * fps / pxpercm\n",
    "df['cum_dist'] = df['cum_dist'] / pxpercm\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "cumul_dist(140,170) / pxpercm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "np.nanmax(df['smoothed_speed']), np.nanmax(df['smoothed_accn'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "## We now remove any outliers that remain post smoothing\n",
    "## Here we want to conservative and not eliminate any relavant points as outliers. We therefore choose a high 'm' value\n",
    "## in the reject_outliers functions. The best approach is to visually compare smoothed data with the original data\n",
    "index = tr.reject_outliers(df['smoothed_speed'], m = 6)\n",
    "index = np.array(index[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.scatter(df['time'][index], df['cum_dist'][index], c='#FF7F50', s=8, alpha=0.5)\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Cumulative distance (cm)')\n",
    "plt.ylim(0,500)\n",
    "plt.tight_layout()\n",
    "plt.savefig(path+'figs/CumulDist.eps', format='eps', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(df['time'][index], df['speed'][index], s=5, alpha=0.5)\n",
    "plt.plot(df['time'][index], df['smoothed_speed'][index], c='#FF7F50', lw=3)\n",
    "plt.ylim(0,10)\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Speed (cm/s)')\n",
    "plt.tight_layout()\n",
    "plt.savefig(path+'figs/Speed.eps', format='eps', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(df['time'][index], df['accn'][index], s=5, alpha=0.5)\n",
    "plt.plot(df['time'][index], df['smoothed_accn'][index], c='#FF7F50', lw=3)\n",
    "plt.ylim(0,1000)\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Acceleration (cm/sq.s)')\n",
    "plt.tight_layout()\n",
    "plt.savefig(path+'figs/Acceleration.eps', format='eps', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#df.drop(df[df.pos_x>700].index, inplace=True)\n",
    "#df.to_csv(output_filepath, sep=',')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python (TrackingAnalysis)",
   "language": "python",
   "name": "trackinganalysis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
